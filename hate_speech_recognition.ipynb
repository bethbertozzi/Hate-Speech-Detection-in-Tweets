{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speech_recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bethbertozzi/Hate-Speech-Detection-in-Tweets/blob/main/hate_speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7JuzBPW266"
      },
      "source": [
        "# Importing Relevant Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIq7NVzAmicq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2179039-61f4-4984-912d-4333feef869f"
      },
      "source": [
        "import sys\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMlQsLuUmDkU"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "!pip3 install fastai\n",
        "import fastai\n",
        "import re\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import string\n",
        "import nltk\n",
        "import warnings \n",
        "import pandas as pd\n",
        "import torchtext\n",
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "import nltk\n",
        "from nltk.stem import *\n",
        "import glob\n",
        "from fastai.text import *\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "import spacy\n",
        "import urllib.parse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iVa_y85P8Jh"
      },
      "source": [
        "contractions = { # https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo4ivDsWX9HY"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbZ0ygD4mqye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac93c452-7814-4ac6-dcb6-f0d89da9cc9e"
      },
      "source": [
        "# Mount gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "project_path = '/content/gdrive/My Drive/APS360/APS360: Team Project/Project' \n",
        "data_path =  '/content/gdrive/My Drive/APS360/APS360: Team Project/Project/Project Data' \n",
        "\n",
        "# For testing\n",
        "dummy_test_path = (data_path + '/hatespeech' + \"/stage2tweets.csv\")\n",
        "dummy_data_set = (pd.read_csv(dummy_test_path, index_col=False)).head(20)\n",
        "\n",
        "sentiment_data_path = data_path +'/CleanedSentiment_NoIdx.csv'\n",
        "hatespeech_data_path = data_path + '/CleanedHateDataCombinedNoIdx.csv'\n",
        "\n",
        "# Sentiment Data\n",
        "sentiment_data = pd.read_csv(sentiment_data_path) # id, label, tweet\n",
        "\n",
        "# Hatespeech Data\n",
        "hatespeech_data = pd.read_csv(hatespeech_data_path)\n",
        "\n",
        "# For testing\n",
        "dummy_test_path = (data_path + '/hatespeech' + \"/stage2tweets.csv\")\n",
        "dummy_data_set = (pd.read_csv(dummy_test_path, index_col=False)).head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrCce-e1Qsge"
      },
      "source": [
        "def check_balance(ds):\n",
        "  percentpos = (sum(ds[\"label\"] == 1)/(ds.shape[0]))*100\n",
        "  percentneg = (sum(ds[\"label\"] == 0)/(ds.shape[0]))*100\n",
        "  print(\"Percent of data that is non-negative: {}% | Percent of data that is negative: {}%\".format(round(percentpos,3),round(percentneg,3)))\n",
        "\n",
        "#check_balance(sentiment_data)\n",
        "#check_balance(hatespeech_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUBGS32IpLV9"
      },
      "source": [
        "**Load Slang Dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV5EM8TTpH5b"
      },
      "source": [
        "dict_path = data_path + \"/slang_dictionary.csv\"\n",
        "slang = pd.read_csv(dict_path).iloc[:, 0:2]\n",
        "print(slang.head())\n",
        "slang_decode = slang.applymap(lambda x: urllib.parse.unquote_plus(x)) # decode url encoding\n",
        "slang_dict = pd.Series(slang_decode.description.values, index=slang_decode.title.values).to_dict()\n",
        "print(slang_dict.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k66ySJNNW7Rg"
      },
      "source": [
        "# Data Pre-Processing\n",
        "\n",
        "**Remove Usernames, Hashtags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vPz-rb6XCeD"
      },
      "source": [
        "def remove_pattern(input_txt, pattern): # INPUT: Text you want to manipulate (str), text you want to remove (str) (Fcn taken from https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/)\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt # OUTPUT: Cleaned text (str)\n",
        "\n",
        "def remove_usernames_hashtags(data_set): # INPUT: Pandas dataframe (with usernames, hashtags)\n",
        "    copy = data_set.copy()\n",
        "    for i in range(copy.shape[0]):\n",
        "      copy.loc[i,\"tweet\"] = remove_pattern(copy.loc[i,\"tweet\"], \"@[\\w]*\") #Removes any word with the first character \"@\"\n",
        "      copy.loc[i,\"tweet\"] = remove_pattern(copy.loc[i,\"tweet\"], \"#\") # Should just removes the hashtag but keep the hashtag word\n",
        "      copy.loc[i,\"tweet\"] = re.sub(r\"http\\S+\", \"\", copy.loc[i,\"tweet\"])\n",
        "      copy.loc[i,\"tweet\"] = remove_pattern(copy.loc[i,\"tweet\"], \"amp\")\n",
        "      if (i%1000 == 0):\n",
        "        print(\"Stage 1 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "    return copy # OUTPUT: New pandas dataframe with usernames removed, hashtags deleted (with usernames, hashtags)\n",
        "\n",
        "def test_remove_usernames_hashtags():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  dummy = dummy.head(10)\n",
        "  print(dummy)\n",
        "  print(remove_usernames_hashtags(dummy))\n",
        "\n",
        "test_remove_usernames_hashtags()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVdFmTYuXCDq"
      },
      "source": [
        "**Replace Slang**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC8TQSewXBnW"
      },
      "source": [
        "def slang_lookup(word):\n",
        "  return slang_dict.get(word.lower(),word) # if word not present in dictionary just return original word\n",
        "\n",
        "def replace_slang(data_set): # INPUT: Pandas dataframe\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]):\n",
        "    words = copy.loc[i,\"tweet\"].split()\n",
        "    slang_free_words = [slang_lookup(word) for word in words]\n",
        "    copy.loc[i,\"tweet\"] = \" \".join(slang_free_words) # put back in a string\n",
        "    if (i%1000 == 0):\n",
        "      print(\"Stage 2 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "  return copy\n",
        "\n",
        "def test_replace_slang():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  slang_dummy = remove_usernames_hashtags(dummy.head(10))\n",
        "  slang_free_dummy = replace_slang(slang_dummy)\n",
        "  print(slang_dummy)\n",
        "  print(slang_free_dummy)\n",
        "\n",
        "test_replace_slang()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-F9kmHyX0UQ"
      },
      "source": [
        "**Remove Short Words, Numbers, Special Characters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMblrLcSX0jp"
      },
      "source": [
        "def replace_words(s, words): # https://stackoverflow.com/questions/20502862/python-replacing-words-in-a-string-with-entries-from-a-dictionary\n",
        "    for k, v in words.items():\n",
        "        s = s.replace(k, v)\n",
        "    return s\n",
        "\n",
        "def remove_contract(data_set): # INPUT: Pandas dataframe, tweets with contractions\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]): \n",
        "    copy.loc[i,\"tweet\"] = replace_words(copy.loc[i,\"tweet\"], contractions)\n",
        "    if (i%1000 == 0):\n",
        "      print(\"Stage 3 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "  return copy # OUTPUT: Pandas dataframe, contractions expanded into separate words\n",
        "\n",
        "def remove_num_sym(data_set): # INPUT: Pandas dataframe with non-alphabetic characters\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]): \n",
        "    copy.loc[i,\"tweet\"] = re.sub(\"[^a-zA-Z]+\", \" \", copy.loc[i,\"tweet\"])\n",
        "    if (i%1000 == 0):\n",
        "      print(\"Stage 4 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "  return copy # OUTPUT: New pandas dataframe with no non-alphabetic characters\n",
        "\n",
        "def remove_short_words(data_set): # INPUT: Pandas dataframe with short words\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]): \n",
        "    copy.loc[i,\"tweet\"] = ' '.join([word for word in copy.loc[i,\"tweet\"].split() if len(word)>=3]) # Get rid of all words that are less than 2 characters long \n",
        "    if (i%1000 == 0):\n",
        "      print(\"Stage 5 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "  return copy # OUTPUT: New pandas dataframe with no words under 3 letters long\n",
        "\n",
        "def remove_capitalization(data_set): # INPUT: Pandas dataframe with tweets with lower and uppercase letters\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]): \n",
        "    copy.loc[i,\"tweet\"] = copy.loc[i,\"tweet\"].lower()\n",
        "    if (i%1000 == 0):\n",
        "      print(\"Stage 6 is {} % complete.\".format((i/copy.shape[0])*100))\n",
        "  return copy # OUTPUT: Pandas dataframe with only lowercase tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIfHtPUCgjZ6"
      },
      "source": [
        "# testing\n",
        "def test_remove_contract():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  no_contracts_dummy = remove_contract(dummy) \n",
        "  print(no_contracts_dummy)\n",
        "\n",
        "def test_remove_num_sym():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  no_num_sym_dummy = remove_num_sym(dummy) \n",
        "  print(no_num_sym_dummy)\n",
        "\n",
        "def test_remove_short_words():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  no_short_words_dummy = remove_short_words(dummy) \n",
        "  print(no_short_words_dummy)\n",
        "\n",
        "def test_remove_capitalization():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  no_caps_dummy = remove_capitalization(dummy) \n",
        "  print(no_caps_dummy)\n",
        "\n",
        "test_remove_contract()\n",
        "test_remove_num_sym()\n",
        "test_remove_short_words()\n",
        "test_remove_capitalization()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcFAn-qXCyW"
      },
      "source": [
        "**Correct Spelling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuiwTWj5XHnK"
      },
      "source": [
        "def correct(data_set): # INPUT: Pandas dataframe with mispellings\n",
        "  spell = SpellChecker()\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]): \n",
        "    tweet = copy.loc[i,\"tweet\"].split()\n",
        "    misspelled = spell.unknown(tweet)\n",
        "    for word in misspelled:\n",
        "      copy.loc[i,\"tweet\"] = re.sub(word, spell.correction(word), copy.loc[i,\"tweet\"]) \n",
        "    if (i%1000==0):\n",
        "      print(\"The spellcheck is \" + str((i/copy.shape[0])*100) + \" complete\")\n",
        "  return copy # OUTPUT: New spellchecked pandas dataframe\n",
        "\n",
        "def test_correct():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  correct_dummy = correct(dummy) \n",
        "  print(correct_dummy)\n",
        "\n",
        "test_correct()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tb57x5dPcrq"
      },
      "source": [
        "**Stem**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHjegh7xPcND"
      },
      "source": [
        "def stem(data_set):\n",
        "  porter = PorterStemmer() # use porter algorithm\n",
        "  copy = data_set.copy()\n",
        "  for i in range(copy.shape[0]):\n",
        "    words = copy.loc[i,\"tweet\"].split()\n",
        "    stemmed_words = [porter.stem(word) for word in words]\n",
        "    copy.loc[i,\"tweet\"] = \" \".join(stemmed_words) # put back in a string\n",
        "  return copy\n",
        "\n",
        "def test_stem():\n",
        "  dummy = dummy_data_set.copy()\n",
        "  stemmed_dummy = stem(dummy)\n",
        "  print(stemmed_dummy)\n",
        "test_stem()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_9e_1NOdzzu"
      },
      "source": [
        "**Combine**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4i9oRYIYWsp"
      },
      "source": [
        "def preprocess(data_set):\n",
        "  print(\"Starting:\")\n",
        "  data_set_clean = remove_usernames_hashtags(data_set)\n",
        "  print(\"Stage one complete.\")\n",
        "  data_set_clean = replace_slang(data_set_clean)\n",
        "  print(\"Stage two complete.\")\n",
        "  data_set_clean = remove_contract(data_set_clean)\n",
        "  print(\"Stage three complete.\")\n",
        "  data_set_clean = remove_num_sym(data_set_clean)\n",
        "  print(\"Stage four complete.\")\n",
        "  data_set_clean = remove_short_words(data_set_clean)\n",
        "  print(\"Stage five complete.\")\n",
        "  data_set_clean = remove_capitalization(data_set_clean)\n",
        "  print(\"Stage six complete.\")\n",
        "  data_set_clean = replace_slang(data_set_clean)\n",
        "  print(\"Stage seven complete.\")\n",
        "  data_set_clean = correct(data_set_clean)\n",
        "  print(\"Stage eight complete.\")\n",
        "  data_set_clean = stem(data_set_clean)\n",
        "  print(\"Finished.\")\n",
        "  return data_set_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icDBOmn6a_-b"
      },
      "source": [
        "**Process Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Zlo9oEOa_mw"
      },
      "source": [
        "# Process Sentiment Data\n",
        "processed_sentiment_data = preprocess(sentiment_data)\n",
        "processed_sentiment_data.to_csv(processed_sentiment_data_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAFF5qghh2uX"
      },
      "source": [
        "# Process Hatespeech Data\n",
        "processed_hatespeech_data = preprocess(hatespeech_data)\n",
        "processed_hatespeech_data.to_csv(processed_hatespeech_data_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3xPfex1FRIp"
      },
      "source": [
        "**Data Analysis for Report:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CD0nJJ2igm3"
      },
      "source": [
        "# Show makeup of data:\n",
        "\n",
        "# Sentiment:\n",
        "check_balance(processed_sentiment_data)\n",
        "# Hatespeech:\n",
        "check_balance(processed_hatespeech_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F84M6XcEGqFI"
      },
      "source": [
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "objects = ('Non-Negative Tweets', 'Negative Tweets')\n",
        "y_pos = np.arange(len(objects))\n",
        "cnt = [sum(processed_sentiment_data[\"label\"] == 1),sum(processed_sentiment_data[\"label\"] == 0)]\n",
        "\n",
        "plt.bar(y_pos, cnt, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.title('Makeup of Sentiment Data')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "objects = ('Hate Speech Tweets', 'Non Hate Speech Tweets')\n",
        "y_pos = np.arange(len(objects))\n",
        "cnt = [sum(processed_hatespeech_data[\"label\"] == 1),sum(processed_hatespeech_data[\"label\"] == 0)]\n",
        "\n",
        "plt.bar(y_pos, cnt, align='center', alpha=0.5)\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Number of Tweets')\n",
        "plt.title('Makeup of Hatespeech Data')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFZ7ffAhFQ0R"
      },
      "source": [
        "# Show at least one example of cleaned training sample.\n",
        "print(\"Uncleaned Tweet: \" + sentiment_data.loc[0,\"tweet\"])\n",
        "print(\"Cleaned Tweet: \" + processed_sentiment_data.loc[1,\"tweet\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glB709VXUIjI"
      },
      "source": [
        "# Reformed Model Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrta0C3pvYwd"
      },
      "source": [
        "**Get Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bI-mLZOiQs59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "932466ad-71c7-42e4-c06d-e8ab9479c092"
      },
      "source": [
        "# Testing non-torch method:\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50, max_vectors=1000000) # use 10k most common words\n",
        "\n",
        "def get_sentiment_data():\n",
        "    return csv.reader(open(sentiment_data_path,\"rt\", encoding=\"latin-1\"))\n",
        "\n",
        "def get_hatespeech_data():\n",
        "    return csv.reader(open(hatespeech_data_path,\"rt\", encoding=\"latin-1\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:32, 2.20MB/s]                          \n",
            " 99%|█████████▉| 396419/400000 [00:10<00:00, 36202.85it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AOh6_SlRVOp",
        "outputId": "5a21175c-c1a7-43ed-f3af-6cae92b1e7b1"
      },
      "source": [
        "def split_tweet(tweet):\n",
        "    # separate punctuations\n",
        "    tweet = tweet.replace(\".\", \" . \") \\\n",
        "                 .replace(\",\", \" , \") \\\n",
        "                 .replace(\";\", \" ; \") \\\n",
        "                 .replace(\"?\", \" ? \")\n",
        "    return tweet.lower().split()\n",
        "\n",
        "split_tweet(\"hello; don't you know?\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', ';', \"don't\", 'you', 'know', '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Kgr76NRwSk",
        "outputId": "96350cdb-113a-478f-bf2e-46f41ba4879e"
      },
      "source": [
        "def split_dataset(glove_vector, get_data):\n",
        "    train, valid, test = [], [], []\n",
        "    for i, line in enumerate(get_data):\n",
        "        if i==3:\n",
        "          print(line)\n",
        "        tweet = line[-1]\n",
        "        idxs = [glove_vector.stoi[w]        # lookup the index of word\n",
        "                for w in split_tweet(tweet)\n",
        "                if w in glove_vector.stoi] # keep words that has an embedding\n",
        "        if not idxs: # ignore tweets without any word with an embedding\n",
        "            continue\n",
        "        idxs = torch.tensor(idxs) # convert list to pytorch tensor\n",
        "        label = torch.tensor(int(line[0]))\n",
        "        if (get_data==get_data_sentiment()):\n",
        "          if i % 2 ==0:\n",
        "              train.append((idxs, label))\n",
        "          else:\n",
        "              valid.append((idxs, label))\n",
        "        else:\n",
        "          if i % 5 < 3:\n",
        "              train.append((idxs, label))\n",
        "          elif i % 5 == 4:\n",
        "              valid.append((idxs, label))\n",
        "          else:\n",
        "              test.append((idxs, label))\n",
        "    return train, valid, test\n",
        "\n",
        "# split datasets into \n",
        "train_sentiment, valid_sentiment, placehold = split_dataset(glove, get_sentiment_data())\n",
        "train_hate, valid_hate, test_hate = split_dataset(glove, get_hatespeech_data())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', 'gregabbott tedcruz first day will rescind every illegal executive action taken barack obama gopdebate foxnews']\n",
            "['0', 'anderson viva based she look like transexual']\n",
            "38949\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(1)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n",
            "tensor(0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCQ_HT8TSFFl"
      },
      "source": [
        "glove_emb = nn.Embedding.from_pretrained(glove.vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC61NI1WvcXv"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjSFIpuyZDvw"
      },
      "source": [
        "class TweetRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(TweetRNN, self).__init__()\n",
        "        self.name = \"TweetRNN\"\n",
        "        self.emb = nn.Embedding.from_pretrained(glove.vectors)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Look up the embedding\n",
        "        x = self.emb(x)\n",
        "        # Set an initial hidden state\n",
        "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        # Forward propagate the RNN\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        # Pass the output of the last time step to the classifier\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge8JgSwuvjMq"
      },
      "source": [
        "**Batching**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Yge1jPZPZZ"
      },
      "source": [
        "import random\n",
        "\n",
        "class TweetBatcher:\n",
        "    def __init__(self, tweets, batch_size=32, drop_last=False):\n",
        "        # store tweets by length\n",
        "        self.tweets_by_length = {}\n",
        "        for words, label in tweets:\n",
        "            # compute the length of the tweet\n",
        "            wlen = words.shape[0]\n",
        "            # put the tweet in the correct key inside self.tweet_by_length\n",
        "            if wlen not in self.tweets_by_length:\n",
        "                self.tweets_by_length[wlen] = []\n",
        "            self.tweets_by_length[wlen].append((words, label),)\n",
        "         \n",
        "        #  create a DataLoader for each set of tweets of the same length\n",
        "        self.loaders = {wlen : torch.utils.data.DataLoader(\n",
        "                                    tweets,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=True,\n",
        "                                    drop_last=drop_last) # omit last batch if smaller than batch_size\n",
        "            for wlen, tweets in self.tweets_by_length.items()}\n",
        "        \n",
        "    def __iter__(self): # called by Python to create an iterator\n",
        "        # make an iterator for every tweet length\n",
        "        iters = [iter(loader) for loader in self.loaders.values()]\n",
        "        while iters:\n",
        "            # pick an iterator (a length)\n",
        "            im = random.choice(iters)\n",
        "            try:\n",
        "                yield next(im)\n",
        "            except StopIteration:\n",
        "                # no more elements in the iterator, remove it\n",
        "                iters.remove(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNUZQ9TdvnwS"
      },
      "source": [
        "**Training Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLA60YjxsJ0B"
      },
      "source": [
        "def get_model_name(name, learning_rate, epoch, stage):\n",
        "    path = \"model_{0}_lr{1}_epoch{2}_stage{3}\".format(name,learning_rate,epoch,stage)\n",
        "    return path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STL-dLAdcDk-"
      },
      "source": [
        "def train_rnn_network(stage, model, train, valid, num_epochs=5, learning_rate=1e-5):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    losses, train_acc, valid_acc = [], [], []\n",
        "    epochs = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for tweets, labels in train:\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(tweets)\n",
        "            loss = criterion(pred, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        losses.append(float(loss))\n",
        "\n",
        "        epochs.append(epoch)\n",
        "        train_acc.append(get_accuracy(model, train))\n",
        "        valid_acc.append(get_accuracy(model, valid))\n",
        "        print(\"Epoch %d; Loss %f; Train Acc %f; Val Acc %f\" % (\n",
        "              epoch+1, loss, train_acc[-1], valid_acc[-1]))\n",
        "        # Save model checkpoint\n",
        "        model_path = get_model_name(model.name, learning_rate, num_epochs, stage)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "    # plotting\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(losses, label=\"Train\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.plot(epochs, train_acc, label=\"Train\")\n",
        "    plt.plot(epochs, valid_acc, label=\"Validation\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pw_TZaj5vq2l"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcNCjDsVcGX3"
      },
      "source": [
        "# Train Stage 1: Sentiment Classification Model\n",
        "sent_model = TweetRNN(50, 100, 2)\n",
        "train_loader_sentiment = TweetBatcher(train_sentiment, batch_size=32, drop_last=True)\n",
        "valid_loader_sentiment = TweetBatcher(valid_sentiment, batch_size=32, drop_last=True)\n",
        "train_rnn_network(1, sent_model, train_loader_sentiment, valid_loader_sentiment, num_epochs=15, learning_rate=2e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UG-mOO9uBgp"
      },
      "source": [
        "# Train Stage 2: Hatespeech Classification Model\n",
        "hate_model = TweetRNN(50, 100, 2)\n",
        "train_loader_hate = TweetBatcher(train_hate, batch_size=16, drop_last=False)\n",
        "valid_loader_hate = TweetBatcher(valid_hate, batch_size=16, drop_last=False)\n",
        "train_rnn_network(2, hate_model, train_loader_hate, valid_loader_hate, num_epochs=30, learning_rate=2e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8BaYHNkO3-J"
      },
      "source": [
        "# Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCxTdGeoScso"
      },
      "source": [
        "def OneStageHatePredictor(tweet, hate_model):\n",
        "  # Process Tweet\n",
        "  tweet = remove_pattern(tweet, \"@[\\w]*\") #Removes any word with the first character \"@\"\n",
        "  tweet = remove_pattern(tweet, \"#\") # Should just removes the hashtag but keep the hashtag word\n",
        "  tweet = replace_words(tweet, contractions)\n",
        "  tweet = re.sub(\"[^a-zA-Z]+\", \" \", tweet)\n",
        "  tweet = ' '.join([word for word in tweet.split() if len(word)>=3])\n",
        "  tweet = tweet.lower()\n",
        "  \n",
        "  # Embed tweet\n",
        "  glove_vector = glove\n",
        "  idx = [glove_vector.stoi[w]        # lookup the index of word\n",
        "          for w in split_tweet(tweet)\n",
        "          if w in glove_vector.stoi] # keep words that has an embedding\n",
        "  idx = torch.tensor(idx) # convert list to pytorch tensor\n",
        "\n",
        "  # Predict \n",
        "  pred2 = hate_model(idx.unsqueeze(0))\n",
        "  max2, indexes = pred2.max(1)\n",
        "  if max2.item() > 0.5:\n",
        "    return(\"Hate\")\n",
        "  else:\n",
        "    return(\"Not Hate\")\n",
        "\n",
        "print(\"This Tweet is...{}\".format(HatePredictor(\"\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWdBLfV4zp_t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "1c10d093-2e78-4435-ad94-3898496c2d88"
      },
      "source": [
        "def OneStageHatePredictorFromTensor(tens, hate_model):\n",
        "  # Predict \n",
        "  pred = sent_model(tens.unsqueeze(0))\n",
        "  max, indexes = pred.max(1)\n",
        "  \n",
        "  if max.item()>1:\n",
        "    pred2 = hate_model(tens.unsqueeze(0))\n",
        "    max2, indexes = pred2.max(1)\n",
        "    if max2.item() > 0.5:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  else:\n",
        "    return 0\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fc84371834ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mHatePredictorFromTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhate_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'none' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef7_T3uwmVGA"
      },
      "source": [
        "def TwoStageHatePredictor(tweet, hate_model, sent_model):\n",
        "# Process Tweet\n",
        "  tweet = remove_pattern(tweet, \"@[\\w]*\") #Removes any word with the first character \"@\"\n",
        "  tweet = remove_pattern(tweet, \"#\") # Should just removes the hashtag but keep the hashtag word\n",
        "  tweet = replace_words(tweet, contractions)\n",
        "  tweet = re.sub(\"[^a-zA-Z]+\", \" \", tweet)\n",
        "  tweet = ' '.join([word for word in tweet.split() if len(word)>=3])\n",
        "  tweet = tweet.lower()\n",
        "  \n",
        "  # Embed tweet\n",
        "  glove_vector = glove\n",
        "  idx = [glove_vector.stoi[w]        # lookup the index of word\n",
        "          for w in split_tweet(tweet)\n",
        "          if w in glove_vector.stoi] # keep words that has an embedding\n",
        "  idx = torch.tensor(idx) # convert list to pytorch tensor\n",
        "\n",
        "  # Load sentiment model\n",
        "  sent_model = TweetRNN(50, 10, 2)\n",
        "  model_path = get_model_name(sent_model.name, 2e-3, 5, 1)\n",
        "  state = torch.load(model_path)\n",
        "  sent_model.load_state_dict(state)\n",
        "\n",
        "  # Load hate model\n",
        "  hate_model = TweetRNN(50, 100, 2)\n",
        "  model_path = get_model_name(hate_model.name, 2e-4, 30, 2)\n",
        "  state = torch.load(model_path) #/content/model_TweetRNN_lr0.002_epoch5_stage1\n",
        "  hate_model.load_state_dict(state)\n",
        "\n",
        "  # Predict \n",
        "  pred = sent_model(idx.unsqueeze(0))\n",
        "  max, indexes = pred.max(1)\n",
        "  if max.item()>1:\n",
        "    pred2 = hate_model(idx.unsqueeze(0))\n",
        "    max2, indexes = pred2.max(1)\n",
        "    if max2.item() > 0.5:\n",
        "      return(\"Hate\")\n",
        "    else:\n",
        "      return(\"Not Hate\")\n",
        "  else:\n",
        "    return(\"Not Hate\")\n",
        "\n",
        "print(\"This Tweet is...{}\".format(TwoStageHatePredictor(\"\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLts5zVkn2k4"
      },
      "source": [
        "def TwoStageHatePredictorFromTensor(tensor, hate_model, sent_model):\n",
        "# Load sentiment model\n",
        "  sent_model = TweetRNN(50, 10, 2)\n",
        "  model_path = get_model_name(sent_model.name, 2e-3, 5, 1)\n",
        "  state = torch.load(model_path)\n",
        "  sent_model.load_state_dict(state)\n",
        "\n",
        "  # Load hate model\n",
        "  hate_model = TweetRNN(50, 100, 2)\n",
        "  model_path = get_model_name(hate_model.name, 2e-4, 30, 2)\n",
        "  state = torch.load(model_path) #/content/model_TweetRNN_lr0.002_epoch5_stage1\n",
        "  hate_model.load_state_dict(state)\n",
        "\n",
        "  # Predict \n",
        "  pred = sent_model(tensor.unsqueeze(0))\n",
        "  max, indexes = pred.max(1)\n",
        "  if max.item()>1:\n",
        "    pred2 = hate_model(tensor.unsqueeze(0))\n",
        "    max2, indexes = pred2.max(1)\n",
        "    if max2.item() > 0.5:\n",
        "      return(\"Hate\")\n",
        "    else:\n",
        "      return(\"Not Hate\")\n",
        "  else:\n",
        "    return(\"Not Hate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHGC6BjXZVIO"
      },
      "source": [
        "def get_accuracy(model, data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets, labels in data_loader:\n",
        "        output = model(tweets)\n",
        "        pred = output.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(labels.view_as(pred)).sum().item()\n",
        "        total += labels.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku1YMnStJ8g-"
      },
      "source": [
        "def get_two_stage_accuracy(data_loader):\n",
        "    correct, total = 0, 0\n",
        "    for tweets,labels in data_loader:\n",
        "      for i in range(len(tweets)):\n",
        "        pred = HatePredictorFromTensor(tweets[i], hate_model)\n",
        "        if(pred == int(labels[i])):\n",
        "          correct +=1\n",
        "      total += labels.shape[0]\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6de5d6g1Sqp"
      },
      "source": [
        "# test accuracy from both stages\n",
        "test_hate_iter = TweetBatcher(test_hate, batch_size=32, drop_last=True)\n",
        "print(get_two_stage_accuracy(test_hate_iter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pcjgQudR4zg",
        "outputId": "59c66b06-66ef-432f-fb77-43064e678050"
      },
      "source": [
        "# test accuracy from only hatespeech model\n",
        "test_hate_iter = TweetBatcher(test_hate, batch_size=32, drop_last=True)\n",
        "print(get_accuracy(hate_model, test_hate_iter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8244964454976303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xO9RV2pkUODf",
        "outputId": "df41cb6c-8432-4fb6-8990-0aba9071ed66"
      },
      "source": [
        "# Confusion Matrix\n",
        "false_pos = 0\n",
        "false_neg = 0\n",
        "true_neg = 0\n",
        "true_pos = 0\n",
        "for tweets,labels in test_hate_iter:\n",
        "  output = model(tweets)\n",
        "  pred = output.max(1, keepdim=True)[1]\n",
        "  for i in range(len(tweets)):\n",
        "    if (int(pred[i]) is 1) and (int(labels[i]) is 1):\n",
        "      true_pos+=1\n",
        "    elif (int(pred[i]) is 1) and (int(labels[i]) is 0):\n",
        "      false_pos+=1\n",
        "    elif (int(pred[i]) is 0) and (int(labels[i]) is 1):\n",
        "      false_neg+=1  \n",
        "    elif (int(pred[i]) is 0) and (int(labels[i]) is 0):\n",
        "      true_neg+=1\n",
        "print(\"false_pos: \" + str(false_pos))\n",
        "print(\"false_neg: \" + str(false_neg))\n",
        "print(\"true_neg: \" + str(true_neg))\n",
        "print(\"true_pos: \" + str(true_pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "false_pos: 511\n",
            "false_neg: 716\n",
            "true_neg: 2992\n",
            "true_pos: 2533\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}