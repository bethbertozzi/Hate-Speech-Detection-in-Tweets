{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "get_tweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4QR73KLXgo5Ja8WD0xyjn"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzkWoBOnurnT"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "current_path = \"/content/gdrive/MyDrive/APS360/APS360: Team Project/Project/Project Data/hatespeech\"\n",
        "\n",
        "#load ids from files\n",
        "import glob\n",
        "import pandas as pd \n",
        "files = glob.glob(current_path + \"/p2p/*/*.csv\")\n",
        "\n",
        "ids = []\n",
        "for file in files:\n",
        "  loaded = pd.read_csv(file)\n",
        "  ids.extend(loaded['tweet_id'].to_list())\n",
        "print(ids) #29619 ids to get \n",
        "tweets = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvld1oCoWD-c"
      },
      "source": [
        "import requests\n",
        "import urllib.parse as urlparse\n",
        "from urllib.parse import urlencode\n",
        "import glob\n",
        "\n",
        "endpoint = \"https://api.twitter.com/2/tweets\"\n",
        "\n",
        "\n",
        "headers = {\"Authorization\": \"\"}\n",
        "paths = []\n",
        "paths = glob.glob(\"./p2p/twitter_hashtag_based_datasets/*.csv\")\n",
        "\n",
        "request = 0\n",
        "while(request < min(300,ceil(len(ids)/100))) # 300 requests per 15 min window\n",
        "  i = request*100\n",
        "  upper_bound = min(i+100, len(ids))\n",
        "  req_ids = ids[i:upper_bound] #get 100 tweets per req\n",
        "  req_ep = add_params(endpoint, req_ids)\n",
        "  r = requests.get(req_ep, headers=headers)\n",
        "  try:\n",
        "    response = r.json()\n",
        "    print(\"successful\")\n",
        "    ogLength = len(tweets)\n",
        "    for tweet in response['data']:\n",
        "      tweets.append(tweet[\"text\"])\n",
        "    #print(\"added \" + str(len(tweets) - ogLength)+ \" to tweets. number of ids requested: \" + str(len(req_ids)))\n",
        "  except: # Unauthorized or deleted tweets\n",
        "    print(r.text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CqVAjTlFauu"
      },
      "source": [
        "import csv\n",
        "with open(current_path + \"/p2p/tweets.csv\", 'w', newline='') as myfile:\n",
        "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
        "    wr.writerow(tweets) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytoqOmS9tJce"
      },
      "source": [
        "def add_params(url, ids): # takes endpoint and list of ids\n",
        "  ids_param = ','.join(map(str, ids))\n",
        "  params = {\"ids\": ids_param}\n",
        "  #print(params)\n",
        "  url_parse = urlparse.urlparse(url)\n",
        "  query = url_parse.query\n",
        "  url_dict = dict(urlparse.parse_qsl(query))\n",
        "  url_dict.update(params)\n",
        "  url_new_query = urlparse.urlencode(url_dict)\n",
        "  url_parse = url_parse._replace(query=url_new_query)\n",
        "  new_url = urlparse.urlunparse(url_parse)\n",
        "\n",
        "  #print(new_url)\n",
        "  return new_url\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}